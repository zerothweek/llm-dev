{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nModel Test\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Model Test\n",
    "''' \n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = \"./EXAONE-3.5-2.4B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map='auto',\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# instruction = \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?\"\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "#     ]\n",
    "\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids,\n",
    "#     max_new_tokens=1024,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9\n",
    "# )\n",
    "\n",
    "# print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\llm_final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = './models/EEVE-Korean-Instruct-10.8B-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, \n",
    "      quantization_config=bnb_config, \n",
    "      device_map='auto',\n",
    "      trust_remote_code = True,\n",
    "  )\n",
    "\n",
    "  model.config.use_cache=False\n",
    "  model.config.pretraining_tp=1\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question)-> str:\n",
    "    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "def generate_response(user_input):\n",
    "  prompt = formatted_prompt(user_input)\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "  generation_config = GenerationConfig(penalty_alpha=0.6,do_sample = True,\n",
    "      top_k=5,temperature=0.5,repetition_penalty=1.2,\n",
    "      max_new_tokens=1024,pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "  start_time = perf_counter()\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  theresponse = (tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  output_time = perf_counter() - start_time\n",
    "  print(f\"Time taken for inference: {round(output_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model=\"finetuningTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_train(input,response)->str:\n",
    "    return f\"<|im_start|>user\\n{input}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1 = [{\"prompt\": \"체계개발실 지능팀 구성원에 대해 알려줘\", \"response\": \"신동헌 팀장님, 김태훈 중위님, 김남현 중위님, 천용 중사님, 신성환 중사님, 최훈진 주무관님, 최준호 병장, 김영주 일병으로 구성되어있습니다.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def prepare_train_datav2(data):\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    data_df = pd.DataFrame(data)\n",
    "    # Create a new column called \"text\"\n",
    "    data_df[\"text\"] = data_df[[\"prompt\", \"response\"]].apply(lambda x: \"<|im_start|>user\\n\" + x[\"prompt\"] + \" <|im_end|>\\n<|im_start|>assistant\\n\" + x[\"response\"] + \"<|im_end|>\\n\", axis=1)\n",
    "    # Create a new Dataset from the DataFrame\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_train_datav2(training_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules = [\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = SFTConfig(\n",
    "        output_dir=output_model,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=16,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=100,\n",
    "        # max_steps=,\n",
    "        fp16=True,\n",
    "        packing = False,\n",
    "        max_seq_length = 1024\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AFOC\\AppData\\Local\\Temp\\ipykernel_21836\\1023881791.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 501.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data,\n",
    "        peft_config=peft_config,\n",
    "        args=arg,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:23<03:18,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7342, 'grad_norm': 3.2032907009124756, 'learning_rate': 0.0001968583161128631, 'epoch': 10.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_final\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_final\\Lib\\site-packages\\transformers\\trainer.py:2529\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2523\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m   2524\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2526\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2529\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2533\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "경기도 수원시에서 아이들과 갈 수 있는 관광 명소 1곳을 알려줘<|im_end|>\n",
      "<|im_start|>assistant: 경기도 수원시에는 가족 단위로 즐길 수 있는 다양한 관광지가 있습니다. 그 중 하나로 **수원 화성**을 추천드립니다!\n",
      "\n",
      "### 수원 화성 (華城)\n",
      "- **위치**: 경기 수원시 팔달구 화서동\n",
      "- **특징**:\n",
      "  - 조선 시대에 건설된 성곽으로, 유네스코 세계문화유산으로 등재되어 있어요.'/>'\n",
      "    + 아름다운 건축물들이 가득하고 역사적 의미도 깊어서 교육적으로 매우 가치있죠。'/></li><ul class=\"list\"><div markdown=\"1\">• **역사 탐방** : 성벽을 따라 걷다 보면 과거의 모습이 생생하게 떠오르고, 다양한 유적 및 전시관 방문 가능</span>><br/><p style=\"\">• **가족 활동 공간** </strong>>><!-- -->* **연등축제나 수원화성국제음악회 등 문화 행사 참여가능합니다.</style-->'</stylediv--><---></body->\n",
      "Time taken for inference: 8.43 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(user_input = \"경기도 수원시에서 아이들과 갈 수 있는 관광 명소 1곳을 알려줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "90253832980c3c6cd087a3988e49febde8d09110ec0c14b52b616cb44c591269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
