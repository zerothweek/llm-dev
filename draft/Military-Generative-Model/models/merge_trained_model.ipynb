{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\llm_final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pretrained_name =  \"./models/EXAONE-3.5-7.8B-Instruct/\"\n",
    "path_to_lora_adapters = \"./models/EXAONE-3.5-7.8B-Instruct-SFT/checkpoint-32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_quant_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:32<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_dtype=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    "    \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_pretrained_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, path_to_lora_adapters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model_name = './TuningModel/checkpoint-100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_pretrained_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(new_model_name, load_in_4bit=True, device_map=\"auto\", trust_remote_code=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######중요######\n",
    "###GPU 초기화###\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"입영일자 연기 제도란 무엇인가요?\"\n",
    "\n",
    "messages = [{\"role\" : \"system\", \"content\" : \"You are EXAONE model from LG AI Research, a helpful assistant.\"}, {\"role\" : \"user\", \"content\" : prompt}]\n",
    "\n",
    "input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input.to(\"cuda\"),\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample = False,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_pretrained_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    path_to_lora_adapters,\n",
    "    torch_dtype = torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_id = './models/EXAONE-3.5-7.8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config = bnb_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39meos_token_id\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]입영일자 연기 신청은 언제까지 가능한가요?\n",
      "[|assistant|]입영일자 연기 신청 기간은 정확히 정해져 있지 않고, **개별 상황에 따라 달라집니다**. \n",
      "\n",
      "일반적으로 다음과 같은 요소들이 연기 신청 가능 기간에 영향을 미칩니다.\n",
      "\n",
      "* **입영 예정일**:  입영일이 다가올수록 연기 신청이 제한될 가능성이 높습니다.\n",
      "* **연기 사유의 타당성**:  병역청이 인정하는 합당한 사유가 있어야 연기가 승인될 가능성이 높습니다.\n",
      "* **병역청의 정책**:  시기별로 병역청의 연기 신청 정책이 변경될 수 있습니다.\n",
      "\n",
      "**정확한 정보는** **병역청 홈페이지**나 **직접 문의**를 통해 확인하시는 것이 가장 좋습니다. \n",
      "\n",
      "LG AI 연구원에서 개발된 EXAONE은 정확한 법률 및 행정 정보 제공에는 한계가 있으므로, 공식적인 출처를 참고하시길 권장합니다.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"입영일자 연기 신청은 언제까지 가능한가요?\"\n",
    "\n",
    "messages = [{\"role\" : \"system\", \"content\" : \"You are EXAONE model from LG AI Research, a helpful assistant.\"}, {\"role\" : \"user\", \"content\" : prompt}]\n",
    "\n",
    "input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input.to(\"cuda\"),\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = False,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]입영일자 연기 신청은 언제까지 가능한가요?\n",
      "[|assistant|]현역병 입영 대상자로서 입영일자 연기를 희망하는 경우, 입영일자 연기신청서를 제출해야 하며, 연기신청은 입영일자 기준 3개월 전까지 가능합니다. 단, 병역이행일 연기사유가 있는 경우, 연기신청은 연기사유 발생일로부터 1년 이내에 제출해야 하며, 연기신청은 연기사유 기준 3개월 전까지 가능합니다. 신청인은 연기사유를 증명할 수 있는 서류를 제출해야 하며, 담당자 확인사항은 없습니다. 신청인은 담당자의 확인사항이 없으므로, 별도의 확인 절차 없이 제출하면 됩니다. 만약 연기신청서를 제출하지 않은 경우, 담당자는 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가 확인사항이 없으므로, 신청인은 연기 여부를 확인할 수 없습니다. 연기 여부는 담당자가\n"
     ]
    }
   ],
   "source": [
    "prompt = \"입영일자 연기 신청은 언제까지 가능한가요?\"\n",
    "\n",
    "messages = [{\"role\" : \"system\", \"content\" : \"You are EXAONE model from LG AI Research, a helpful assistant.\"}, {\"role\" : \"user\", \"content\" : prompt}]\n",
    "\n",
    "input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input.to(\"cuda\"),\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = False,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90253832980c3c6cd087a3988e49febde8d09110ec0c14b52b616cb44c591269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
