{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\llm_final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Jun_13_19:42:34_Pacific_Daylight_Time_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.91\n",
      "Build cuda_12.2.r12.2/compiler.32965470_0\n",
      "Name: torch\n",
      "Version: 2.3.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: C:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, mkl, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, autoawq, autoawq_kernels, bitsandbytes, peft, sentence-transformers, torchaudio, torchvision\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m   1779\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_processing_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration_auto\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\image_processing_utils.py:21\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_processing_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_transforms\u001b[39;00m \u001b[39mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ChannelDimension\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\image_transforms.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     ChannelDimension,\n\u001b[0;32m     24\u001b[0m     ImageInput,\n\u001b[0;32m     25\u001b[0m     get_channel_dimension_axis,\n\u001b[0;32m     26\u001b[0m     get_image_size,\n\u001b[0;32m     27\u001b[0m     infer_channel_dimension_format,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\image_utils.py:58\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m is_torchvision_available():\n\u001b[1;32m---> 58\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m InterpolationMode\n\u001b[0;32m     60\u001b[0m     pil_torch_interpolation_mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m         PILImageResampling\u001b[39m.\u001b[39mNEAREST: InterpolationMode\u001b[39m.\u001b[39mNEAREST,\n\u001b[0;32m     62\u001b[0m         PILImageResampling\u001b[39m.\u001b[39mBOX: InterpolationMode\u001b[39m.\u001b[39mBOX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         PILImageResampling\u001b[39m.\u001b[39mLANCZOS: InterpolationMode\u001b[39m.\u001b[39mLANCZOS,\n\u001b[0;32m     67\u001b[0m     }\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS  \u001b[39m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[39m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:163\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m grad\u001b[39m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[1;32m--> 163\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39;49mlibrary\u001b[39m.\u001b[39;49mregister_fake(\u001b[39m\"\u001b[39m\u001b[39mtorchvision::nms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[0;32m    165\u001b[0m     torch\u001b[39m.\u001b[39m_check(dets\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39mlambda\u001b[39;00m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mboxes should be a 2d tensor, got \u001b[39m\u001b[39m{\u001b[39;00mdets\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39mD\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain_huggingface\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatHuggingFace, HuggingFacePipeline\n\u001b[0;32m      4\u001b[0m \u001b[39m# 로컬 모델 경로 설정\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[39m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[0;32m   1767\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "\n",
    "# 로컬 모델 경로 설정\n",
    "local_model_path = \"./llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "# HuggingFace pipeline 생성\n",
    "hf_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=local_model_path,\n",
    "    config=f\"{local_model_path}\\\\config.json\",\n",
    "    tokenizer=f\"{local_model_path}\\\\tokenizer.json\",\n",
    "    device=0,  # GPU 사용, CPU를 사용하려면 -1로 설정\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# HuggingFacePipeline 생성\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ChatHuggingFace 생성\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatHuggingFace 사용 예제\n",
    "response = chat_model.generate([\"Hello! How are you?\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      4\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./EXAONE-3.5-2.4B-Instruct\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      7\u001b[0m     model_name,\n\u001b[0;32m      8\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[0;32m      9\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     10\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:553\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39mif\u001b[39;00m has_remote_code \u001b[39mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    552\u001b[0m     class_ref \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mauto_map[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n\u001b[1;32m--> 553\u001b[0m     model_class \u001b[39m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    554\u001b[0m         class_ref, pretrained_model_name_or_path, code_revision\u001b[39m=\u001b[39;49mcode_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    555\u001b[0m     )\n\u001b[0;32m    556\u001b[0m     _ \u001b[39m=\u001b[39m hub_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    557\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:552\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m    540\u001b[0m final_module \u001b[39m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    541\u001b[0m     repo_id,\n\u001b[0;32m    542\u001b[0m     module_file \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m     repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[0;32m    551\u001b[0m )\n\u001b[1;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload\u001b[39m=\u001b[39;49mforce_download)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:249\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[1;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, \u001b[39m\"\u001b[39m\u001b[39m__transformers_module_hash__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m module_hash:\n\u001b[1;32m--> 249\u001b[0m     module_spec\u001b[39m.\u001b[39;49mloader\u001b[39m.\u001b[39;49mexec_module(module)\n\u001b[0;32m    250\u001b[0m     module\u001b[39m.\u001b[39m__transformers_module_hash__ \u001b[39m=\u001b[39m module_hash\n\u001b[0;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\EXAONE-3.5-2.4B-Instruct\\modeling_exaone.py:45\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     BaseModelOutputWithPast,\n\u001b[0;32m     39\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     SequenceClassifierOutputWithPast,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_rope_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ROPE_INIT_FUNCTIONS\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ALL_LAYERNORM_LAYERS\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     49\u001b[0m     add_start_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     logging,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeneration\u001b[39;00m \u001b[39mimport\u001b[39;00m GenerationConfig, GenerationMixin\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mintegrations\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloss\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloss_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquantizers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoHfQuantizer, HfQuantizer\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloss_deformable_detr\u001b[39;00m \u001b[39mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloss_for_object_detection\u001b[39;00m \u001b[39mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloss_rt_detr\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDetrForObjectDetectionLoss\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\loss\\loss_deformable_detr.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_transforms\u001b[39;00m \u001b[39mimport\u001b[39;00m center_to_corners_format\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_scipy_available\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mloss_for_object_detection\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     HungarianMatcher,\n\u001b[0;32m      8\u001b[0m     ImageLoss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     sigmoid_focal_loss,\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\image_transforms.py:22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterable, List, Optional, Tuple, Union\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimage_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     ChannelDimension,\n\u001b[0;32m     24\u001b[0m     ImageInput,\n\u001b[0;32m     25\u001b[0m     get_channel_dimension_axis,\n\u001b[0;32m     26\u001b[0m     get_image_size,\n\u001b[0;32m     27\u001b[0m     infer_channel_dimension_format,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     is_flax_available,\n\u001b[0;32m     32\u001b[0m     is_tf_available,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     requires_backends,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\image_utils.py:58\u001b[0m\n\u001b[0;32m     55\u001b[0m         PILImageResampling \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39mImage\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m is_torchvision_available():\n\u001b[1;32m---> 58\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m InterpolationMode\n\u001b[0;32m     60\u001b[0m         pil_torch_interpolation_mapping \u001b[39m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m             PILImageResampling\u001b[39m.\u001b[39mNEAREST: InterpolationMode\u001b[39m.\u001b[39mNEAREST,\n\u001b[0;32m     62\u001b[0m             PILImageResampling\u001b[39m.\u001b[39mBOX: InterpolationMode\u001b[39m.\u001b[39mBOX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m             PILImageResampling\u001b[39m.\u001b[39mLANCZOS: InterpolationMode\u001b[39m.\u001b[39mLANCZOS,\n\u001b[0;32m     67\u001b[0m         }\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS  \u001b[39m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[39m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[39mreturn\u001b[39;00m fn\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[39m@register_meta\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mroi_align\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mmeta_roi_align\u001b[39;49m(\u001b[39minput\u001b[39;49m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[0;32m     27\u001b[0m     torch\u001b[39m.\u001b[39;49m_check(rois\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39;49m \u001b[39m5\u001b[39;49m, \u001b[39mlambda\u001b[39;49;00m: \u001b[39m\"\u001b[39;49m\u001b[39mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m     torch\u001b[39m.\u001b[39;49m_check(\n\u001b[0;32m     29\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdtype \u001b[39m==\u001b[39;49m rois\u001b[39m.\u001b[39;49mdtype,\n\u001b[0;32m     30\u001b[0m         \u001b[39mlambda\u001b[39;49;00m: (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m         ),\n\u001b[0;32m     34\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\Lib\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mif\u001b[39;00m torchvision\u001b[39m.\u001b[39;49mextension\u001b[39m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[39m.\u001b[39mimpl(\u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고 민수가 남은 철수의 연필 중 5개를 가져갔으면 각각 몇개의 연필을 가지고있는가?\n",
      "[|assistant|]철수의 초기 연필 개수는 20개였습니다.\n",
      "\n",
      "1. **영희가 가져간 연필 수**: 철수의 연필의 절반이므로,\n",
      "   \\[\n",
      "   \\frac{20}{2} = 10 \\text{개}\n",
      "   \\]\n",
      "   영희는 철수로부터 10개의 연필을 가져갔습니다.\n",
      "\n",
      "2. **철수가 남은 연필 수**: 초기 연필에서 영희가 가져간 연필을 뺀 값입니다.\n",
      "   \\[\n",
      "   20 - 10 = 10 \\text{개}\n",
      "   \\]\n",
      "   철수는 남은 연필이 10개입니다.\n",
      "\n",
      "3. **민수가 가져간 연필 수**: 철수가 남은 연필 중 5개를 가져갔으므로,\n",
      "   \\[\n",
      "   10 - 5 = 5 \\text{개}\n",
      "   \\]\n",
      "   민수는 철수로부터 5개의 연필을 가져갔습니다.\n",
      "\n",
      "따라서, 각 사람이 가지고 있는 연필 개수는 다음과 같습니다:\n",
      "- 철수: **5개**\n",
      "- 영희: **10개**\n",
      "- 민수: **5개**[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./models/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"  # English example\n",
    "prompt = \"스스로를 자랑해 봐\"       # Korean example\n",
    "prompt = \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고 민수가 남은 철수의 연필 중 5개를 가져갔으면 각각 몇개의 연필을 가지고있는가?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "90253832980c3c6cd087a3988e49febde8d09110ec0c14b52b616cb44c591269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
