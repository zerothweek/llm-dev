{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.expanduser(\"~/.env_global\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Get base model dir from environment, fallback to default\n",
    "base_model_dir = os.getenv('MODEL_DIR', '/home/models')\n",
    "\n",
    "def get_model_path(model_name):\n",
    "    return os.path.join(base_model_dir, model_name)\n",
    "\n",
    "# Example model\n",
    "model_name = 'Llama-3.2-1B'\n",
    "model_path = get_model_path(model_name)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "A General way to visualize the output token probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'models/Llama-3.2-1B/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/utils/hub.py:342\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'models/Llama-3.2-1B/'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mmodels/Llama-3.2-1B/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:881\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m    880\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m    883\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:713\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    710\u001b[39m     token = use_auth_token\n\u001b[32m    712\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/utils/hub.py:408\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    409\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncorrect path_or_model_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    410\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[31mOSError\u001b[39m: Incorrect path_or_model_id: 'models/Llama-3.2-1B/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = 'models/Llama-3.2-1B/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞에 파이프라인 보여줘도 될듯 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eiffel tower is located in the heart of Paris, France.'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model_name, device=device)\n",
    "output = pipe(\"Eiffel tower is located in\", max_new_tokens=7)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `(1) Untitled`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_txt:\n",
      "Wake up in the\n",
      "\n",
      "input_tokens:\n",
      "['<|begin_of_text|>', 'Wake', 'Ġup', 'Ġin', 'Ġthe']\n",
      "\n",
      "input_ids:\n",
      "tensor([[128000,  91848,    709,    304,    279]])\n",
      "\n",
      "model_input:\n",
      "<|begin_of_text|>Wake up in the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\" # model's input text in the user view \n",
    "input_tokens = tokenizer.tokenize(input_txt, add_special_tokens=True) # the input text tokenized with <begin token> added\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device) # the actual model's input\n",
    "model_input = tokenizer.decode(input_ids[0]) # what the model's input look like in user(understanding) view\n",
    "\n",
    "print(f'input_txt:\\n{input_txt}\\n')\n",
    "print(f'input_tokens:\\n{input_tokens}\\n')\n",
    "print(f'input_ids:\\n{input_ids}\\n')\n",
    "print(f'model_input:\\n{model_input}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_logits torch.Size([1, 5, 128256]):\n",
      "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
      "         [11.4998,  7.3979,  6.4436,  ..., -1.8154, -1.8148, -1.8155],\n",
      "         [18.2506,  9.9784,  7.8313,  ...,  0.6443,  0.6445,  0.6443],\n",
      "         [10.4916,  7.3949,  5.6094,  ..., -0.7362, -0.7360, -0.7363],\n",
      "         [ 8.2426,  6.4981,  4.8060,  ..., -0.5330, -0.5322, -0.5321]]])\n",
      "\n",
      "next_token_logits torch.Size([128256]):\n",
      "tensor([ 8.2426,  6.4981,  4.8060,  ..., -0.5330, -0.5322, -0.5321])\n",
      "\n",
      "next_token_probs torch.Size([128256]):\n",
      "tensor([1.0895e-06, 1.9037e-07, 3.5055e-08,  ..., 1.6828e-10, 1.6842e-10,\n",
      "        1.6843e-10])\n",
      "\n",
      "next_possible_token_ids torch.Size([128256]):\n",
      "tensor([  6693,   6278,  29084,  ...,  64422, 124977, 107790])\n",
      "\n",
      "output_txt:\n",
      " morning (64.77%)       middle (1.91%)       Morning (1.31%)       early (1.26%)       heart (1.03%)      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids)\n",
    "\n",
    "'''callout\n",
    "-output logits are the final output of the model \n",
    "before applying any activation function like softmax\n",
    "-output logit tensor shape: (batch_size, sequence_length, vocab_size)\n",
    "- What does the output logits tensor mean?\n",
    "    -> the raw scores for each token in the vocabulary\n",
    "    (which later gets converted to the probability of being the next token) for each input_tokens'''\n",
    "output_logits = output.logits # final output of the model\n",
    "next_token_logits = output.logits[0, -1, :] # raw scores of the next possible token based on the input sequence\n",
    "\n",
    "'''callout\n",
    "At inference time we're only interested in the last output_logits \n",
    "'''\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1) # probability of the next possible token based on the input sequence\n",
    "next_possible_token_ids = torch.argsort(next_token_probs, dim=-1, descending=True) # the next possible tokens id ordered in descending \n",
    "\n",
    "output_txt = ''# top 5 possible model output text with probability in user view\n",
    "for choice_idx in range(5):\n",
    "    token_id = next_possible_token_ids[choice_idx]\n",
    "    token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "    output_txt += f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)      \"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"output_logits {output_logits.shape}:\\n{output_logits}\\n\")\n",
    "print(f\"next_token_logits {next_token_logits.shape}:\\n{next_token_logits}\\n\")\n",
    "print(f\"next_token_probs {next_token_probs.shape}:\\n{next_token_probs}\\n\")\n",
    "print(f\"next_possible_token_ids {next_possible_token_ids.shape}:\\n{next_possible_token_ids}\\n\")\n",
    "print(f\"output_txt:\\n{output_txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing 1.The iterative decoding process of the language model 2. The models possible output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in</td>\n",
       "      <td>the (42.77%)</td>\n",
       "      <td>a (14.96%)</td>\n",
       "      <td>style (5.29%)</td>\n",
       "      <td>your (1.48%)</td>\n",
       "      <td>time (1.37%)</td>\n",
       "      <td>the (42.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the</td>\n",
       "      <td>morning (64.77%)</td>\n",
       "      <td>middle (1.91%)</td>\n",
       "      <td>Morning (1.31%)</td>\n",
       "      <td>early (1.26%)</td>\n",
       "      <td>heart (1.03%)</td>\n",
       "      <td>morning (64.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning</td>\n",
       "      <td>and (40.07%)</td>\n",
       "      <td>, (18.15%)</td>\n",
       "      <td>with (9.96%)</td>\n",
       "      <td>to (9.65%)</td>\n",
       "      <td>. (2.54%)</td>\n",
       "      <td>and (40.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and</td>\n",
       "      <td>you (9.84%)</td>\n",
       "      <td>get (2.98%)</td>\n",
       "      <td>see (2.93%)</td>\n",
       "      <td>go (2.80%)</td>\n",
       "      <td>have (2.78%)</td>\n",
       "      <td>you (9.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and you</td>\n",
       "      <td>will (10.41%)</td>\n",
       "      <td>’ll (8.46%)</td>\n",
       "      <td>are (7.85%)</td>\n",
       "      <td>’re (7.35%)</td>\n",
       "      <td>can (5.77%)</td>\n",
       "      <td>will (10.41%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>find (18.20%)</td>\n",
       "      <td>be (17.60%)</td>\n",
       "      <td>see (14.46%)</td>\n",
       "      <td>have (5.06%)</td>\n",
       "      <td>feel (4.72%)</td>\n",
       "      <td>find (18.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>yourself (22.32%)</td>\n",
       "      <td>that (13.18%)</td>\n",
       "      <td>a (11.09%)</td>\n",
       "      <td>your (9.35%)</td>\n",
       "      <td>the (7.70%)</td>\n",
       "      <td>yourself (22.32%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>in (25.02%)</td>\n",
       "      <td>at (4.94%)</td>\n",
       "      <td>on (4.09%)</td>\n",
       "      <td>surrounded (4.03%)</td>\n",
       "      <td>with (3.48%)</td>\n",
       "      <td>in (25.02%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>a (36.20%)</td>\n",
       "      <td>the (27.58%)</td>\n",
       "      <td>an (5.10%)</td>\n",
       "      <td>front (4.47%)</td>\n",
       "      <td>your (2.61%)</td>\n",
       "      <td>a (36.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>world (5.06%)</td>\n",
       "      <td>beautiful (4.90%)</td>\n",
       "      <td>new (3.51%)</td>\n",
       "      <td>room (3.09%)</td>\n",
       "      <td>place (2.63%)</td>\n",
       "      <td>world (5.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>of (37.84%)</td>\n",
       "      <td>where (19.81%)</td>\n",
       "      <td>that (15.22%)</td>\n",
       "      <td>full (7.18%)</td>\n",
       "      <td>filled (2.81%)</td>\n",
       "      <td>of (37.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>endless (1.72%)</td>\n",
       "      <td>your (1.67%)</td>\n",
       "      <td>beauty (1.43%)</td>\n",
       "      <td>possibilities (1.26%)</td>\n",
       "      <td>beautiful (1.14%)</td>\n",
       "      <td>endless (1.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>possibilities (44.12%)</td>\n",
       "      <td>opportunities (5.75%)</td>\n",
       "      <td>beauty (2.18%)</td>\n",
       "      <td>options (2.05%)</td>\n",
       "      <td>choices (1.19%)</td>\n",
       "      <td>possibilities (44.12%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>. (59.73%)</td>\n",
       "      <td>, (7.59%)</td>\n",
       "      <td>and (6.51%)</td>\n",
       "      <td>.\\n (5.40%)</td>\n",
       "      <td>! (2.78%)</td>\n",
       "      <td>. (59.73%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>You (12.95%)</td>\n",
       "      <td>The (8.17%)</td>\n",
       "      <td>There (4.69%)</td>\n",
       "      <td>It (4.14%)</td>\n",
       "      <td>This (2.90%)</td>\n",
       "      <td>You (12.95%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>can (37.26%)</td>\n",
       "      <td>will (15.21%)</td>\n",
       "      <td>have (11.73%)</td>\n",
       "      <td>are (9.65%)</td>\n",
       "      <td>may (4.20%)</td>\n",
       "      <td>can (37.26%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>choose (13.34%)</td>\n",
       "      <td>start (6.60%)</td>\n",
       "      <td>go (5.86%)</td>\n",
       "      <td>wake (4.09%)</td>\n",
       "      <td>be (4.03%)</td>\n",
       "      <td>choose (13.34%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>to (59.12%)</td>\n",
       "      <td>from (10.16%)</td>\n",
       "      <td>the (5.29%)</td>\n",
       "      <td>between (4.75%)</td>\n",
       "      <td>your (3.82%)</td>\n",
       "      <td>to (59.12%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>wake (11.39%)</td>\n",
       "      <td>be (8.96%)</td>\n",
       "      <td>go (8.09%)</td>\n",
       "      <td>start (6.27%)</td>\n",
       "      <td>live (4.87%)</td>\n",
       "      <td>wake (11.39%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;|begin_of_text|&gt;Wake up in the morning and yo...</td>\n",
       "      <td>up (98.52%)</td>\n",
       "      <td>in (0.23%)</td>\n",
       "      <td>-up (0.22%)</td>\n",
       "      <td>to (0.17%)</td>\n",
       "      <td>and (0.16%)</td>\n",
       "      <td>up (98.52%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Input  \\\n",
       "0                         <|begin_of_text|>Wake up in   \n",
       "1                     <|begin_of_text|>Wake up in the   \n",
       "2             <|begin_of_text|>Wake up in the morning   \n",
       "3         <|begin_of_text|>Wake up in the morning and   \n",
       "4     <|begin_of_text|>Wake up in the morning and you   \n",
       "5   <|begin_of_text|>Wake up in the morning and yo...   \n",
       "6   <|begin_of_text|>Wake up in the morning and yo...   \n",
       "7   <|begin_of_text|>Wake up in the morning and yo...   \n",
       "8   <|begin_of_text|>Wake up in the morning and yo...   \n",
       "9   <|begin_of_text|>Wake up in the morning and yo...   \n",
       "10  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "11  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "12  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "13  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "14  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "15  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "16  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "17  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "18  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "19  <|begin_of_text|>Wake up in the morning and yo...   \n",
       "\n",
       "                   Choice 1                Choice 2          Choice 3  \\\n",
       "0              the (42.77%)              a (14.96%)     style (5.29%)   \n",
       "1          morning (64.77%)          middle (1.91%)   Morning (1.31%)   \n",
       "2              and (40.07%)              , (18.15%)      with (9.96%)   \n",
       "3               you (9.84%)             get (2.98%)       see (2.93%)   \n",
       "4             will (10.41%)             ’ll (8.46%)       are (7.85%)   \n",
       "5             find (18.20%)             be (17.60%)      see (14.46%)   \n",
       "6         yourself (22.32%)           that (13.18%)        a (11.09%)   \n",
       "7               in (25.02%)              at (4.94%)        on (4.09%)   \n",
       "8                a (36.20%)            the (27.58%)        an (5.10%)   \n",
       "9             world (5.06%)       beautiful (4.90%)       new (3.51%)   \n",
       "10              of (37.84%)          where (19.81%)     that (15.22%)   \n",
       "11          endless (1.72%)            your (1.67%)    beauty (1.43%)   \n",
       "12   possibilities (44.12%)   opportunities (5.75%)    beauty (2.18%)   \n",
       "13               . (59.73%)               , (7.59%)       and (6.51%)   \n",
       "14             You (12.95%)             The (8.17%)     There (4.69%)   \n",
       "15             can (37.26%)           will (15.21%)     have (11.73%)   \n",
       "16          choose (13.34%)           start (6.60%)        go (5.86%)   \n",
       "17              to (59.12%)           from (10.16%)       the (5.29%)   \n",
       "18            wake (11.39%)              be (8.96%)        go (8.09%)   \n",
       "19              up (98.52%)              in (0.23%)       -up (0.22%)   \n",
       "\n",
       "                  Choice 4            Choice 5                   Output  \n",
       "0             your (1.48%)        time (1.37%)             the (42.77%)  \n",
       "1            early (1.26%)       heart (1.03%)         morning (64.77%)  \n",
       "2               to (9.65%)           . (2.54%)             and (40.07%)  \n",
       "3               go (2.80%)        have (2.78%)              you (9.84%)  \n",
       "4              ’re (7.35%)         can (5.77%)            will (10.41%)  \n",
       "5             have (5.06%)        feel (4.72%)            find (18.20%)  \n",
       "6             your (9.35%)         the (7.70%)        yourself (22.32%)  \n",
       "7       surrounded (4.03%)        with (3.48%)              in (25.02%)  \n",
       "8            front (4.47%)        your (2.61%)               a (36.20%)  \n",
       "9             room (3.09%)       place (2.63%)            world (5.06%)  \n",
       "10            full (7.18%)      filled (2.81%)              of (37.84%)  \n",
       "11   possibilities (1.26%)   beautiful (1.14%)          endless (1.72%)  \n",
       "12         options (2.05%)     choices (1.19%)   possibilities (44.12%)  \n",
       "13             .\\n (5.40%)           ! (2.78%)               . (59.73%)  \n",
       "14              It (4.14%)        This (2.90%)             You (12.95%)  \n",
       "15             are (9.65%)         may (4.20%)             can (37.26%)  \n",
       "16            wake (4.09%)          be (4.03%)          choose (13.34%)  \n",
       "17         between (4.75%)        your (3.82%)              to (59.12%)  \n",
       "18           start (6.27%)        live (4.87%)            wake (11.39%)  \n",
       "19              to (0.17%)         and (0.16%)              up (98.52%)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Wake up in\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 20\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Store tokens with highest probabiities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        \n",
    "\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=1)\n",
    "        iteration['Output'] = iteration['Choice 1']\n",
    "        iterations.append(iteration)\n",
    "\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Greedy Search Decoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Wake up in the morning and you will find yourself in a world of endless possibilities. You can choose to wake up and\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=20, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Beam Search Decoding`\n",
    "(greedy in a different way  \n",
    "reference: [NLP using huggingface]p.130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf. \n",
    "In model.generate(), the difference between max_new_tokens and max_length is:\n",
    "\n",
    "1. max_length: This defines the total length of the generated sequence, including the input prompt and the newly generated tokens. If max_length=100 and your input already has 50 tokens, the model will generate at most 50 new tokens.\n",
    "\n",
    "\n",
    "2. max_new_tokens: This defines only the number of new tokens to generate, without counting the input tokens. If max_new_tokens=50, the model will generate 50 new tokens regardless of the prompt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Wake up in the morning to the sound of birds chirping and the smell of freshly brewed coffee. Wake up to the'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "output_beam = model.generate(input_ids, max_new_tokens=20, num_beams=5,do_sample=False)\n",
    "tokenizer.decode(output_beam[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `temperature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Wake up in the top Ful'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "output_temp = model.generate(input_ids, max_new_tokens=2, do_sample=True, temperature=2.0, top_k=0)\n",
    "tokenizer.decode(output_temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `top_k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Wake up in the morning,'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "output_topk = model.generate(input_ids, max_new_tokens=2, do_sample=True, top_k=10)\n",
    "tokenizer.decode(output_topk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Wake up in the morning and'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_txt = \"Wake up in the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "output_topp = model.generate(input_ids, max_new_tokens=2, do_sample=True, top_p=0.90)\n",
    "tokenizer.decode(output_topp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction of Temperature, Top-k, and Top-p in Text Generation\n",
    "\n",
    "1. **Temperature (T)**\n",
    "   - Controls randomness in token selection.\n",
    "   - **Higher T (>1.0)** → More diverse output (flatter probability distribution).\n",
    "   - **Lower T (<1.0)** → More deterministic (sharper probability peaks).\n",
    "\n",
    "2. **Top-k Sampling**\n",
    "   - Keeps only the **k most probable** tokens.\n",
    "   - Ignores all other tokens, even if they contribute to probability mass.\n",
    "   - **Effect:** Hard threshold on how many tokens can be considered.\n",
    "\n",
    "3. **Top-p (Nucleus) Sampling**\n",
    "   - Selects tokens dynamically until their **cumulative probability** reaches **p**.\n",
    "   - **Effect:** Adjusts the number of tokens based on probability distribution.\n",
    "\n",
    "###### 🔥 **Using All Three Together (`T + top-k + top-p`)**\n",
    "   1. **Temperature (T) adjusts the probability distribution** (spreads out or concentrates probabilities).\n",
    "   2. **Top-k removes all but the k highest probability tokens**.\n",
    "   3. **Top-p further narrows down the selection** to tokens covering **p% of the probability mass**.\n",
    "   \n",
    "⚠️ **Key Effects & Considerations**\n",
    "   - **If `top-k` is small & `top-p` is large** → `top-k` dominates, as `top-p` can't add more choices.  \n",
    "   - **If `top-k` is large & `top-p` is small** → `top-p` dominates, as it removes unlikely tokens even if they are within `top-k`.  \n",
    "   - **If both `top-k` and `top-p` are too restrictive** → Repetitive or deterministic output.  \n",
    "   - **Temperature influences both methods** → Higher `T` makes rare tokens more likely, affecting `top-k` and `top-p` choices.\n",
    "\n",
    "✅ **Example of Balanced Usage**\n",
    "```python\n",
    "model.generate(\n",
    "    input_ids, \n",
    "    temperature=0.8,  # Adds diversity but keeps coherence\n",
    "    top_k=50,         # Limits selection to top 50 tokens\n",
    "    top_p=0.9         # Keeps only tokens contributing to 90% probability mass\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
