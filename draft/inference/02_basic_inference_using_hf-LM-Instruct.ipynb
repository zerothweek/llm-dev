{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, \\\n",
    "    BitsAndBytesConfig, GenerationConfig\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "# Automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "device, _, _ = get_backend()\n",
    "# Define the base model directory and model ID\n",
    "MODEL_DIR = '/home/zerothweek/llm/models'\n",
    "model_id = 'Llama-3.2-1B-Instruct'\n",
    "#model_id = 'SmolLM2-135M-Instruct'\n",
    "# Construct the full path to the model\n",
    "model_path = os.path.join(MODEL_DIR, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Setting model\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_comput_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    #quantization_config = quantization_config,\n",
    "    trust_remote_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Setting tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Generationg Config\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference One Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference One Input\n",
    "def inference_single(messages):\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True,return_tensors=\"pt\").to(device) # tokenize model input\n",
    "\n",
    "    #generated_ids = model.generate(model_inputs, generation_config=generation_config, max_new_tokens=1) # inference model\n",
    "    generated_ids = model.generate(model_inputs, do_sample=False)\n",
    "    model_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0] # decode model output\n",
    "    return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Apr 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrr, me hearty! I be Captain Blackbeak, the most feared and infamous pirate to'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inference_single(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 05 Apr 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrr, me hearty! I be Captain Blackbeak, the most feared and infamous pirate to'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
    "  warnings.warn(\n",
    "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
    "  warnings.warn(\n",
    "\n",
    "\n",
    "Arrr, me hearty! I be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the seven seas. Me and me trusty parrot, Polly, be sailin' the Caribbean, plunderin' and pillagin' wherever we go. Me ship, the \"Maverick's Revenge\", be a sturdy galleon, with three masts and a hull as black as coal. Me crew be a motley bunch o' scurvy dogs, but they be loyal to me to the death. We be sailin' the seas, searchin' for treasure and avoidin' the authorities. So hoist the colors, me hearties, and set course for adventure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Batch Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_batch(chats):\n",
    "    texts = [tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ) for chat in chats]\n",
    "    print(texts)\n",
    "    model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    print(model_inputs)\n",
    "    generated_ids = model.generate(**model_inputs, generation_config=generation_config) # inference model\n",
    "    model_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=False) # decode model output\n",
    "    return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Mar 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Mar 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat year is it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n']\n",
      "{'input_ids': tensor([[128009, 128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,\n",
      "          33025,   2696,     25,   6790,    220,   2366,     18,    198,  15724,\n",
      "           2696,     25,    220,   1591,   2947,    220,   2366,     20,    271,\n",
      "           2675,    527,    264,  55066,   6369,   6465,    889,   2744,  31680,\n",
      "            304,  55066,   6604,      0, 128009, 128006,    882, 128007,    271,\n",
      "          15546,    527,    499,     30, 128009, 128006,  78191, 128007,    271],\n",
      "        [128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   1591,   2947,    220,   2366,     20,    271,   2675,\n",
      "            527,    264,  55066,   6369,   6465,    889,   2744,  31680,    304,\n",
      "          55066,   6604,      0, 128009, 128006,    882, 128007,    271,   3923,\n",
      "           1060,    374,    433,     30, 128009, 128006,  78191, 128007,    271]]), 'attention_mask': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|eot_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Mar 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrr, me hearty! Yer lookin' fer me, eh? Me name be Captain Black\",\n",
       " \"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Mar 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat year is it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYer lookin' fer the year, eh? Alright then, matey! It be \"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chats = [\n",
    "    [{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "\n",
    "    [{\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What year is it?\"}]\n",
    "]\n",
    "inference_batch(chats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
