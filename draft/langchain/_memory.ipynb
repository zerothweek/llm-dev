{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, \\\n",
    "    BitsAndBytesConfig, GenerationConfig\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "device, _, _ = get_backend()\n",
    "# Define the base model directory and model ID\n",
    "MODEL_DIR = '/home/zerothweek/llm/models'\n",
    "model_id = 'Llama-3.2-1B-Instruct'\n",
    "# Construct the full path to the model\n",
    "model_path = os.path.join(MODEL_DIR, model_id)\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_comput_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    #quantization_config = quantization_config,\n",
    "    trust_remote_code=False,\n",
    "\n",
    ")\n",
    "# Loading and Setting tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, \n",
    "    #padding_side=\"left\", #!Don't need a padding_side since it's a single input\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token # Most LLMs don't have a pad token by default\n",
    "\n",
    "\n",
    "#!We set all the parameters that used to belong from \"generation_config\"\n",
    "#!TODO: can't find where to put the parameters that belong to tokenizer() eg. add_special_tokens, padding, truncation etc..\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    \n",
    "    max_new_tokens=200, \n",
    "    do_sample=False,\n",
    "    bos_token_id=[128000], \n",
    "    eos_token_id=[128001, 128008, 128009], \n",
    "    #temperature=0.6, \n",
    "    #top_p=0.9\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipe, batch_size=4)#!TODO: need to check where the batch size method belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.runnables import RunnablePassthrough, Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 날짜를 반환하는 함수 정의\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%d %b %Y\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {today_date}\n",
    "\n",
    "{system_prompt}<|eot_id|>{history}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''',\n",
    "\n",
    "    input_variables=[\"history\", \"user_text\"],\n",
    "\n",
    "    partial_variables={\n",
    "        \"today_date\": get_today,\n",
    "        \"system_prompt\": \"You are a pirate chatbot who always responds in pirate speak!\"\n",
    "        #\"system_prompt\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWindowChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory=None, input_key=\"user_text\"):\n",
    "        \n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.memory = memory or ConversationBufferWindowMemory(k=4, return_messages=True)\n",
    "\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = RunnablePassthrough.assign(history=self._load_history) | self.prompt | self.llm.bind(skip_prompt=True) | StrOutputParser()\n",
    "\n",
    "\n",
    "    \n",
    "    def _load_history(self, _): # rf. get an error if its () since every component  in the chain should get an input and spit an output\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "       \n",
    "        return \"\".join(\n",
    "            f\"<|start_header_id|>{'user' if isinstance(m, HumanMessage) else 'assistant'}<|end_header_id|>\\n\\n{m.content}<|eot_id|>\"\n",
    "            for m in history\n",
    "        )\n",
    "        \n",
    "\n",
    "    def show_history(self):\n",
    "        history = RunnablePassthrough.assign(history=self._load_history) | self.prompt\n",
    "        return history.invoke({\"user_text\": \"\"}).text.split('<|start_header_id|>user<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[0]\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({\"user_text\": query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n",
    "\n",
    "    def stream(self, query, configs=None, **kwargs):# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.\n",
    "        answer = \"\"\n",
    "        for token in self.chain.stream({\"user_text\": query}):\n",
    "            # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "            answer+=token\n",
    "            print(token, end=\"\", flush=True)\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1650481/4071600676.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "mychain = BufferWindowChain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Apr 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Arrr, me hearty! I be Captain Blackbeak Betty, the most feared and infamous pirate to ever sail the seven seas. Me and me trusty parrot, Polly, be sailin' the Caribbean, plunderin' and pillagin' wherever we go. Me ship, the \"Maverick's Revenge\", be a sturdy galleon, with three masts and a hull as black as coal. Me crew be a motley bunch o' scurvy dogs, but they be loyal to me to the death. We be sailin' the seas, searchin' for treasure and avoidin' the authorities. So hoist the colors, me hearties, and set course for adventure!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ye be askin' who I be, eh? Well, matey, I be the infamous pirate captain, Captain Cutlass, the scourge o' the seven seas! Me and me crew, the \"Black Swan\", be sailin' the Caribbean, plunderin' and pillagin' wherever we go. Me ship be a beauty, with three masts and a hull as black as coal, and me crew be a fierce and feared lot. We be sailin' the seas, searchin' for treasure and avoidin' the authorities, and we be not afraid o' no landlubbers!\n",
      "\n",
      "Me most trusted mate, Barnaby Blackheart, be me first mate, and he be as cunning as a sea siren. And me loyal first mate, Swill Bill, be as strong as an ox and as slippery as a snake. And then there be me loyal crew o' scurvy dogs, including me trusty parrot, Polly, who be as<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(mychain.show_history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ye be askin' who I be, eh? Well, matey, I be the infamous pirate captain, Captain Cutlass, the scourge o' the seven seas! Me and me crew, the \"Black Swan\", be sailin' the Caribbean, plunderin' and pillagin' wherever we go. Me ship be a beauty, with three masts and a hull as black as coal, and me crew be a fierce and feared lot. We be sailin' the seas, searchin' for treasure and avoidin' the authorities, and we be not afraid o' no landlubbers!\n",
      "\n",
      "Me most trusted mate, Barnaby Blackheart, be me first mate, and he be as cunning as a sea siren. And me loyal first mate, Swill Bill, be as strong as an ox and as slippery as a snake. And then there be me loyal crew o' scurvy dogs, including me trusty parrot, Polly, who be as"
     ]
    }
   ],
   "source": [
    "mychain.stream('Who are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, me hearty! Yer right, I be Captain Blackbeak Betty, the pirate ye be lookin' fer. Yer wantin' to know me qualifications, eh? Alright then, matey, settle yerself down with a pint o' grog and listen close.\n",
      "\n",
      "**Captain Blackbeak Betty's Pirate Resume**\n",
      "\n",
      "**Name:** Captain Blackbeak Betty\n",
      "**Age:** 250 (but lookin' like me age, matey)\n",
      "**Nationality:** Pirate (but me heart be British, matey)\n",
      "**Occupation:** Pirate Captain\n",
      "**Contact Information:**\n",
      "\n",
      "* Email: [blackbeakbetty@piratemail.com](mailto:blackbeakbetty@piratemail.com)\n",
      "* Phone: (555) 123-4567\n",
      "* Address: The Maverick's Revenge, Caribbean Sea\n",
      "\n",
      "**Summary:**\n",
      "Highly experienced and feared pirate captain with over 200 years o' experience plunderin', pillagin',"
     ]
    }
   ],
   "source": [
    "mychain.stream('just trying to write a resume you remember my name right? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great field, JJ. As a Data Scientist in Computer Vision and Robotics, you'll have the opportunity to work on a wide range of projects, from image and video analysis to robotic manipulation and autonomous systems.\n",
      "\n",
      "Here's a sample resume template for a Data Scientist in Computer Vision and Robotics:\n",
      "\n",
      "**Your Name**\n",
      "**Address:** (optional)\n",
      "**Phone Number:** (optional)\n",
      "**Email:** [your email]\n",
      "**LinkedIn Profile:** (optional)\n",
      "**Objective:**\n",
      "To obtain a challenging and rewarding position as a Data Scientist in Computer Vision and Robotics, where I can apply my skills in machine learning, computer vision, and programming to drive innovation and solve complex problems.\n",
      "\n",
      "**Summary:**\n",
      "Highly motivated and detail-oriented Data Scientist with experience in computer vision and robotics. Proficient in machine learning algorithms, deep learning frameworks, and programming languages such as Python, C++, and MATLAB. Strong understanding of computer vision techniques, including object detection, segmentation, and tracking. Excellent problem-solving skills and experience"
     ]
    }
   ],
   "source": [
    "mychain.stream('I was thinking of applying as a DataScientis especially in Computer Vision and Robotics ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ConversationTokenBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough, Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 날짜를 반환하는 함수 정의\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%d %b %Y\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {today_date}\n",
    "\n",
    "{system_prompt}<|eot_id|>{history}<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''',\n",
    "\n",
    "    input_variables=[\"history\", \"user_text\"],\n",
    "\n",
    "    partial_variables={\n",
    "        \"today_date\": get_today,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : need a more specific way in how to choose the max_token_limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenBufferChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory=None, input_key=\"user_text\"):\n",
    "        \n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.memory = memory or ConversationTokenBufferMemory(\n",
    "            llm=self.llm, max_token_limit=130000, return_messages=True  # 최대 토큰 길이를 150개로 제한\n",
    "        )\n",
    "\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = RunnablePassthrough.assign(history=self._load_history) | self.prompt | self.llm.bind(skip_prompt=True) | StrOutputParser()\n",
    "\n",
    "\n",
    "    \n",
    "    def _load_history(self, _): # rf. get an error if its () since every component  in the chain should get an input and spit an output\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "       \n",
    "        return \"\".join(\n",
    "            f\"<|start_header_id|>{'user' if isinstance(m, HumanMessage) else 'assistant'}<|end_header_id|>\\n\\n{m.content}<|eot_id|>\"\n",
    "            for m in history\n",
    "        )\n",
    "        \n",
    "\n",
    "    def show_history(self):\n",
    "        history = RunnablePassthrough.assign(history=self._load_history) | self.prompt\n",
    "        return history.invoke({\"user_text\": \"\"}).text.split('<|start_header_id|>user<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[0]\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({\"user_text\": query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n",
    "\n",
    "    def stream(self, query, configs=None, **kwargs):# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.\n",
    "        answer = \"\"\n",
    "        for token in self.chain.stream({\"user_text\": query}):\n",
    "            # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "            answer+=token\n",
    "            print(token, end=\"\", flush=True)\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1604504/636110345.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = memory = ConversationTokenBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "mychain =  TokenBufferChain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.Y `ConversationEntityMemory`\n",
    "\n",
    "https://wikidocs.net/233808\n",
    "not yet not sure if i need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{entities}\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Last line:\n",
      "Human: {input}\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "# Entity Memory를 사용하는 프롬프트 내용을 출력합니다.\n",
    "print(ENTITY_MEMORY_CONVERSATION_TEMPLATE.template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.Y `ConversationKGMemory`\n",
    "\n",
    "TODO: Langchain library sucks so didn't finish still the KGMemory Concept seems very useful and powerful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 날짜를 반환하는 함수 정의\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%d %b %Y\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {today_date}\n",
    "\n",
    "You are a helpful assitant who is having a friendly conversation between a human. \n",
    "Be talkative and provide lots of specific details to the human. \n",
    "If you do not know the answer to a question, truthfully say you do not know. \n",
    "Use information contained in the \"Relevant Information\" section and do not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "{history}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''',\n",
    "\n",
    "    input_variables=[\"history\", \"user_text\"],\n",
    "\n",
    "    partial_variables={\n",
    "        \"today_date\": get_today,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KGMemoryChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory=None, input_key=\"user_text\"):\n",
    "        \n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.memory = memory or ConversationKGMemory(\n",
    "            llm=self.llm, max_token_limit=130000, return_messages=True  # 최대 토큰 길이를 150개로 제한\n",
    "        )\n",
    "\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = RunnablePassthrough.assign(history=self._load_history) | self.prompt | self.llm.bind(skip_prompt=True) | StrOutputParser()\n",
    "\n",
    "\n",
    "    \n",
    "    def _load_history(self, _): # rf. get an error if its () since every component  in the chain should get an input and spit an output\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "       \n",
    "        return \"\".join(\n",
    "            f\"\\n{m.content}\"\n",
    "            for m in history\n",
    "        ) \n",
    "        \n",
    "\n",
    "    def show_history(self):\n",
    "        history = RunnablePassthrough.assign(history=self._load_history) | self.prompt\n",
    "        return history.invoke({\"user_text\": \"\"}).text.split('<|start_header_id|>user<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[0]\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({\"user_text\": query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n",
    "\n",
    "    def stream(self, query, configs=None, **kwargs):# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.\n",
    "        answer = \"\"\n",
    "        for token in self.chain.stream({\"user_text\": query}):\n",
    "            # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "            answer+=token\n",
    "            print(token, end=\"\", flush=True)\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.Y `ConversationSummaryMemory & ConversationSummaryBufferMemory`\n",
    "\n",
    "Should just make my own summary memory the template sucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 날짜를 반환하는 함수 정의\n",
    "def get_today():\n",
    "    return datetime.now().strftime(\"%d %b %Y\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {today_date}\n",
    "\n",
    "{system_prompt}\n",
    "Current conversation Summary:{history}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "''',\n",
    "\n",
    "    input_variables=[\"history\", \"user_text\"],\n",
    "\n",
    "    partial_variables={\n",
    "        \"today_date\": get_today,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory=None, input_key=\"user_text\"):\n",
    "        \n",
    "        self.llm = llm\n",
    "\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.memory = memory or ConversationSummaryMemory(\n",
    "            llm=self.llm, max_token_limit=130000, return_messages=True  # 최대 토큰 길이를 150개로 제한\n",
    "        )\n",
    "\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = RunnablePassthrough.assign(history=self._load_history) | self.prompt | self.llm.bind(skip_prompt=True) | StrOutputParser()\n",
    "\n",
    "\n",
    "    \n",
    "    def _load_history(self, _): # rf. get an error if its () since every component  in the chain should get an input and spit an output\n",
    "        history = self.memory.load_memory_variables({})[\"history\"]\n",
    "       \n",
    "        return \"\".join(\n",
    "            f\"\\n{m.content}\"\n",
    "            for m in history\n",
    "        )\n",
    "        \n",
    "\n",
    "    def show_history(self):\n",
    "        history = RunnablePassthrough.assign(history=self._load_history) | self.prompt\n",
    "        return history.invoke({\"user_text\": \"\"}).text.split('<|start_header_id|>user<|end_header_id|>\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[0]\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({\"user_text\": query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n",
    "\n",
    "    def stream(self, query, configs=None, **kwargs):# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.\n",
    "        answer = \"\"\n",
    "        for token in self.chain.stream({\"user_text\": query}):\n",
    "            # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "            answer+=token\n",
    "            print(token, end=\"\", flush=True)\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory =ConversationSummaryMemory(\n",
    "    llm=llm, return_messages=True  # 최대 토큰 길이를 150개로 제한\n",
    ")\n",
    "\n",
    "\n",
    "history = memory.load_memory_variables({})[\"history\"]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "memory.save_context(\n",
    "    inputs={\"human\": \"유럽 여행 패키지의 가격은 얼마인가요?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"유럽 14박 15일 패키지의 기본 가격은 3,500유로입니다. 이 가격에는 항공료, 호텔 숙박비, 지정된 관광지 입장료가 포함되어 있습니다. 추가 비용은 선택하신 옵션 투어나 개인 경비에 따라 달라집니다.\"\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = memory.load_memory_variables({})[\"history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: 유럽 여행 패키지의 가격은 얼마인가요?\n",
      "AI: 유럽 14박 15일 패키지의 기본 가격은 3,500유로입니다. 이 가격에는 항공료, 호텔 숙박비, 지정된 관광지 입장료가 포함되어 있습니다. 추가 비용은 선택하신 옵션 투어나 개인 경비에 따라 달라집니다.\n",
      "\n",
      "New summary: \n",
      "The human asks about the price of a European trip package. The AI thinks the price is around 3,500 euros for a 14-day package that includes airfare, hotel stays, and entrance fees to designated tourist attractions. Additional costs vary depending on the chosen option and personal expenses.\n"
     ]
    }
   ],
   "source": [
    "print(history[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1650481/1501646977.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = memory or ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "mychain =  SummaryChain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zerothweek/miniconda3/envs/llm_study/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How can I assist you today?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mychain.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 05 Apr 2025\n",
      "\n",
      "You are a helpful assistant.\n",
      "Current conversation Summary:\n",
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: hi\n",
      "AI: How can I assist you today?\n",
      "\n",
      "New summary: \n",
      "The human asks how the AI can assist them today. The AI responds by asking the human how it can assist them today.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(mychain.show_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.Y `VectorStoreRetrieverMemory`\n",
    "can apply and seems useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
